{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metropolis-Hasting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Assume $\\mathbf{X}=(X_1,\\dots,X_{10})$ follows a multivariate standard normal distribution $\\pi(\\mathbf{x})$. Build a Random Walk algorithm with the Gaussian kernel to estimate $E[D]$, where $D=f(\\mathbf{X})=\\sqrt{\\sum_{j=1}^{10}X_j^2}$. Firstly, tune the step size such that you have roughly 23.4\\% acceptance rate. What step size do you choose? Then, draw 10000 samples to do the estimation, set the initial point $\\mathbf{x}_0=\\mathbf{0}$, set the burn-in parameter to be 1000 and set the thinning parameter to be 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RW(step, target, burn, x0, size, thin):\n",
    "    def move(x_old):\n",
    "        x_new = x_old + st.multivariate_normal(mean=np.zeros(10), cov=step ** 2).rvs()\n",
    "        return x_new if st.uniform.rvs() <= (target(x_new) / target(x_old)) else x_old\n",
    "    \n",
    "    for b in range(burn):\n",
    "        x0 = move(x0)\n",
    "    \n",
    "    x = np.zeros([size, 10])\n",
    "    x[0] = x0\n",
    "    for i in range(size - 1):\n",
    "        for j in range(thin):\n",
    "            x0 = move(x0)\n",
    "        \n",
    "        x[i + 1] = x0\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate: 0.2334\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(19971107)\n",
    "target = st.multivariate_normal(mean=np.zeros(10)).pdf\n",
    "X = RW(step=0.8, target=target, burn=0, x0=np.zeros(10), size=10000, thin=1)\n",
    "D = np.sqrt(np.sum(X ** 2, axis=1))\n",
    "print('acceptance rate:', len(set(D)) / D.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate: 3.086575980864723\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(19971107)\n",
    "target = st.multivariate_normal(mean=np.zeros(10)).pdf\n",
    "X = RW(step=0.8, target=target, burn=1000, x0=np.zeros(10), size=10000, thin=5)\n",
    "D = np.sqrt(np.sum(X ** 2, axis=1))\n",
    "estimate = D.mean()\n",
    "print('estimate:', estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) A simple techique called batching can help us to build a confidence interval with MCMC samples. We divide the above obtained 10000 samples $\\{d_1,\\dots,d_{10000}\\}$ into 20 batches, where $d_i=f(\\mathbf{x}_i)$, and $\\textbf{x}_i=(x_{i1},\\dots,x_{i10})$ is a sample in the MCMC sequence. Do the estimation in each batch:\n",
    "$$\n",
    "\\overline{d}_b=\\frac{1}{500}\\sum_{i=500(b-1)+1}^{500b}d_i,\n",
    "$$\n",
    "for $b=1,\\dots,20$. Estimate that\n",
    "$$\n",
    "s^2=\\frac{1}{20(20-1)}\\sum_{b=1}^{20}(\\overline{d}_b-\\overline{d})^2. \n",
    "$$\n",
    "So, the 95\\% confidence interval would be\n",
    "$$\n",
    "\\overline{d}\\pm t_{(19)}^{0.975}s. \n",
    "$$\n",
    "Please give this interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batching(samples, B):\n",
    "    averages = np.zeros(B)\n",
    "    size = int(samples.size / B)\n",
    "    for b in range(B):\n",
    "        averages[b] = samples[size * b: size * (b + 1)].mean()\n",
    "        \n",
    "    average = averages.mean()\n",
    "    s = np.sqrt(np.var(averages) / (B - 1))\n",
    "    ts = st.t(df=B - 1).ppf(0.975) * s\n",
    "    print('95% C.I.: [{}, {}]'.format(average - ts, average + ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% C.I.: [3.0480739092203764, 3.125078052509069]\n"
     ]
    }
   ],
   "source": [
    "batching(D, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) In the self-normalized IS, the optimal proposal is propotional to $\\pi(\\mathbf{x})|f(\\mathbf{x})-E[D]|$. Replace $E[D]$ by the estimate $\\hat{E}[D]$ obtained in (a), draw samples from the optimal proposal based on the Random Walk, and weight each obtained sample by the weighting function \n",
    "$$\n",
    "w(\\mathbf{x})=\\frac{1}{|f(\\mathbf{x})-\\hat{E}[D]|}. \n",
    "$$\n",
    "Follow a similar procedure of (a) and (b) to calculate the estimate of $E[D]$ and the 95\\% confidence interval by the self-normalized IS. Does your confidence interval become wider or narrower? (hints: (1) Remember to ensure the 23.4\\% acceptance rate; (2) You don't need to consider the weights of the averages for each batch in this question. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate: 0.2352\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(19971107)\n",
    "target2 = lambda x: st.multivariate_normal(mean=np.zeros(10)).pdf(x) * np.abs(np.sqrt(np.sum(x ** 2)) - estimate)\n",
    "X2 = RW(step=0.8, target=target2, burn=0, x0=np.zeros(10), size=10000, thin=1)\n",
    "D2 = np.sqrt(np.sum(X2 ** 2, axis=1))\n",
    "print('acceptance rate:', len(set(D2)) / D2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate: 3.083841117561734\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(19971107)\n",
    "target = st.multivariate_normal(mean=np.zeros(10)).pdf\n",
    "X2 = RW(step=0.8, target=target2, burn=1000, x0=np.zeros(10), size=10000, thin=5)\n",
    "D2 = np.sqrt(np.sum(X ** 2, axis=1))\n",
    "W = 1 / np.abs(D2 - estimate)\n",
    "estimate2 = np.sum(W * D2) / np.sum(W)\n",
    "print('estimate:', estimate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batching(samples, weights, B):\n",
    "    averages = np.zeros(B)\n",
    "    size = int(samples.size / B)\n",
    "    for b in range(B):\n",
    "        w = weights[size * b: size * (b + 1)]\n",
    "        averages[b] = np.sum(w * samples[size * b: size * (b + 1)]) / np.sum(w)\n",
    "        \n",
    "    average = averages.mean()\n",
    "    s = np.sqrt(np.var(averages) / (B - 1))\n",
    "    ts = st.t(df=B - 1).ppf(0.975) * s\n",
    "    print('95% C.I.: [{}, {}]'.format(average - ts, average + ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% C.I.: [3.077353218409959, 3.0894742759791107]\n"
     ]
    }
   ],
   "source": [
    "batching(D2, W, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
